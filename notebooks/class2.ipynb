{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6522dbfa",
   "metadata": {},
   "source": [
    "# Graph Machine Learning Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2efa87",
   "metadata": {},
   "source": [
    "by Alejandro Correa Bahnsen, Jaime D. Acevedo-Viloria & Luisa Roa\n",
    "\n",
    "version 1.2, October 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1cac9",
   "metadata": {},
   "source": [
    "In this notebook we will be doing a brief introduction to graph machine learning. The agenda is as follows:\n",
    "\n",
    "\n",
    "1. Different types of graphs - Introduction to NetworkX\n",
    "2. Creating Graph Based Features and enhancing ML models\n",
    "3. Creating a Graph from own data using NetworkX\n",
    "4. Transductive Learning vs. Inductive Learning\n",
    "5. Graph Neural Networks - Introduction to DGL\n",
    "\n",
    "Through these basics you will be able to leverage graphs for the enhacement of Machine Learning models, or be able to learn the basics of how to build Neural Networks specially crafted for Graphs. We hope you like it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install libraries\n",
    "!pip install dgl==0.6.1\n",
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eb36d",
   "metadata": {},
   "source": [
    "## Types of Graphs - An Introduction to NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee69b25",
   "metadata": {},
   "source": [
    "NetworkX according to it's creators is: NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. \n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/networkx_description.png)\n",
    "\n",
    "https://networkx.org/\n",
    "\n",
    "We will be using the latest stable 2.8.7 version of the Package as referenced on the requirements .txt provided in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8343044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd87c3",
   "metadata": {},
   "source": [
    "Let's start by describing different properties graphs can have and what those mean for the graph in subject. We will use NetworkX visual examples for every one of them and we will also describe real world applications where you may find such type of graph.\n",
    "\n",
    "We will be using different NetworkX to innitialize graphs, this is just to highlight the many different ways we can do this. Make sure to check the documentation for more info in this: https://networkx.org/documentation/stable/reference/generators.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af8b14",
   "metadata": {},
   "source": [
    "### Directed & Undirected Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e856bd",
   "metadata": {},
   "source": [
    "This property refers as to whether the edges connecting the graphs have an inherent direction in it.\n",
    "\n",
    "In undirected the graphs, edges indicate a two-way relationship, and as such they can be traversed from either node to other connected. In directed graphs, edges indicate a one-way direction. Meaning that they can only be traversed in an specific direction of the edge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4a7bd",
   "metadata": {},
   "source": [
    "## Creating Graph Based Features and enhancing ML models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4772b",
   "metadata": {},
   "source": [
    "Now let's see how we can use these new features taken from the graph to enhance our ML models. First, let's import the information of the users in the graph from the nodes_features_workshop csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/jdacevedo3010/graph-mahine-learning-workshop/raw/master/data/nodes_features_workshop.csv').set_index('USER_ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b41975",
   "metadata": {},
   "source": [
    "Here we have a DataFrame with the users as the index. The columns contain the features that profile them:\n",
    "\n",
    "1. Device Type: An encoding of the different devices in the dataset\n",
    "2. Expected Value: A score that measures the value the client will bring\n",
    "3. Sales: Total amount spent by the user\n",
    "\n",
    "And a label that tells us whether the user is fraudulent or not (FRAUD column).\n",
    "\n",
    "Let's use this information to train a couple of traditional Machine Learning models such as: Gradient Boosting Trees, and Logistic Regression. Let's first create train and test masks, we will do this manually given that we wil later need this same division for the Graph Neural Networks. We will also be using torch tensors for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d266d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def create_masks(df, seed=23, test_size=0.2):\n",
    "    '''\n",
    "    This function creates binary tensors that indicate whether an user is on the train or test set\n",
    "    '''\n",
    "    np.random.RandomState(seed)\n",
    "    temp_df = df.copy()\n",
    "    temp_df['split_flag'] = np.random.random(df.shape[0])\n",
    "    train_mask = th.BoolTensor(np.where(temp_df['split_flag'] <= (1 - test_size), True, False))\n",
    "    test_mask = th.BoolTensor(np.where((1 - test_size) < temp_df['split_flag'] , True, False))\n",
    "    return train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary masks\n",
    "train_mask, test_mask = create_masks(df, 23, 0.3)\n",
    "\n",
    "print(train_mask)\n",
    "\n",
    "#Here we transform the tensors so they indicate the indices of the train and test users instead of the binary\n",
    "train_nid = train_mask.nonzero().squeeze()\n",
    "test_nid = test_mask.nonzero().squeeze()\n",
    "\n",
    "print(train_nid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4c6d0",
   "metadata": {},
   "source": [
    "Now, let's create the X and Y tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and Y dataframes\n",
    "X = df.drop(['FRAUD'], axis=1)\n",
    "y = df.drop(['DEVICE_TYPE','EXPECTED_VALUE','SALES'], axis=1)\n",
    "\n",
    "print('The shape of the X DataFrame is: ',X.shape)\n",
    "print('The shape of the y DataFrame is: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ff17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the X and Y dataframes to tensors now as well\n",
    "X = th.tensor(X.values).float()\n",
    "y = th.tensor(y.values).type(th.LongTensor).squeeze_()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d17495",
   "metadata": {},
   "source": [
    "Let's create the functions to train the ML models, and a function that allows us to measure the performance of those models in terms of ROC Curve AUC, F1-Score, Precision and Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def get_gb_preds(X_train, y_train, X_test, seed=23):\n",
    "    clf = GradientBoostingClassifier(random_state=seed)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred_probas = clf.predict_proba(X_test)\n",
    "    return y_pred_probas\n",
    "\n",
    "def get_lr_preds(X_train, y_train, X_test, seed=23):\n",
    "    clf = LogisticRegression(random_state=seed)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred_probas = clf.predict_proba(X_test)\n",
    "    return y_pred_probas\n",
    "\n",
    "def get_results(y_pred_probas, y_test, threshold=0.5):\n",
    "    pred_probas_1 = y_pred_probas[:,1]\n",
    "    preds_1 = np.where(pred_probas_1>threshold,1,0)\n",
    "    auc = roc_auc_score(y_test, pred_probas_1)\n",
    "    f1 = f1_score(y_test,preds_1)\n",
    "    prec = precision_score(y_test,preds_1)\n",
    "    recall = recall_score(y_test,preds_1)\n",
    "    return auc, f1, prec, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e94bca",
   "metadata": {},
   "source": [
    "#### Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043be43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[train_nid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'logistic-regression'\n",
    "y_pred_probas = get_lr_preds(X[train_nid], y[train_nid], X[test_nid], seed=23)\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Model','AUC','F1 Score','Precision','Recall'])\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623b31b",
   "metadata": {},
   "source": [
    "#### GBoost results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GBoost'\n",
    "y_pred_probas = get_gb_preds(X[train_nid], y[train_nid], X[test_nid], seed=23)\n",
    "\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138f6f3",
   "metadata": {},
   "source": [
    "## Creating a Graph from own data using NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a7b8d",
   "metadata": {},
   "source": [
    "We will know see how to create a graph from data instead of randomly. For this we will have to import the csv's in the data folder and process them for NetworkX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a961e",
   "metadata": {},
   "source": [
    "First lets import the relations csv in the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "edges_df = pd.read_csv('https://github.com/jdacevedo3010/graph-mahine-learning-workshop/raw/master/data/new_edges_workshop.csv')\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53582be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20e03b",
   "metadata": {},
   "source": [
    "Here you a 2-colummn DataFrame that contains the undirected edges between distinct users. This is normally referred to as \"List of edges\" and it's a common way to create graphs. NetworkX also has a method to create a graph from a DataFrame of edges, let's do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf4f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edges_df,'~from','~to')\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27856d06",
   "metadata": {},
   "source": [
    "Let's draw a portion of the graph to check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_draw = nx.from_pandas_edgelist(edges_df.head(100),'~from','~to')\n",
    "nx.draw(G_draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6165be",
   "metadata": {},
   "source": [
    "Nice! We can now use this created graph to extract characteristics from it. \n",
    "\n",
    "Let's say we want to get the number of neighbors of each node in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G.degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401329e",
   "metadata": {},
   "source": [
    "We can use the degree method in NetworkX to create a dictionary that holds the node id's as the keys and the degree of that node as the value. There are also plenty of other measures, we can extract from the Graph object of NetworkX like centrality or betweeness metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1237cc",
   "metadata": {},
   "source": [
    "More information about the NetworkX library can be found in this tutorial: https://networkx.org/nx-guides/content/tutorial.html. Or in the overall documentation guide of the package: https://networkx.org/nx-guides/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02facc",
   "metadata": {},
   "source": [
    "#### And now let's enhance the features with some extracted from the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcecb90",
   "metadata": {},
   "source": [
    "Now we will use the previously generated dictionary of degrees as an additional feature to the DataFrame, along with the centrality measure PageRank.\n",
    "\n",
    "PageRank was developed by Google and measures the importance of a node in a Graph given how connected are the node's neighbors. More information can be found here: https://es.wikipedia.org/wiki/PageRank\n",
    "\n",
    "Let's first calculate that for the previously generated graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ba92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(G,alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36223be7",
   "metadata": {},
   "source": [
    "And now let's add both the degree and PageRank as features for the users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map Degree and PageRank into the DataFrame\n",
    "df_enhanced = df.copy()\n",
    "df_enhanced['DEGREE'] = df.index.map(degrees)\n",
    "df_enhanced['PAGERANK'] = df.index.map(pr)\n",
    "\n",
    "df_enhanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ba0b6",
   "metadata": {},
   "source": [
    "## Now let's add a personalized feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_score_dict = df_enhanced.DEVICE_TYPE.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a834dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, device_score_dict, 'device_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_attribute(G, u, attribute_name):\n",
    "    return sum(G.nodes[v]['device_score'] for v in G.neighbors(u)) / G.degree(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_device_score_dict = { u: average_attribute(G,u, 'device_score') for u in G.nodes }\n",
    "\n",
    "df_enhanced['AVG_NEIGHBOR_DV_SCORE'] = df.index.map(avg_device_score_dict)\n",
    "\n",
    "df_enhanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14256e",
   "metadata": {},
   "source": [
    "And finally, let's run the same models as before with these new features to see how the results compare with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and Y dataframes\n",
    "X_enhanced = df_enhanced.drop(['FRAUD'], axis=1).fillna(0)\n",
    "y_enhanced = df_enhanced.drop(['DEVICE_TYPE','EXPECTED_VALUE','SALES','DEGREE','PAGERANK','AVG_NEIGHBOR_DV_SCORE'], axis=1)\n",
    "\n",
    "print('The shape of the X DataFrame is: ',X.shape)\n",
    "print('The shape of the y DataFrame is: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6734b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_enhanced[['DEGREE','PAGERANK','AVG_NEIGHBOR_DV_SCORE']] = scaler.fit_transform(X_enhanced[['DEGREE','PAGERANK','AVG_NEIGHBOR_DV_SCORE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11456462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the X and Y dataframes to tensors now as well\n",
    "X_enhanced = th.tensor(X_enhanced.values).float()\n",
    "y_enhanced = th.tensor(y_enhanced.values).type(th.LongTensor).squeeze_()\n",
    "\n",
    "print(X_enhanced.shape)\n",
    "print(y_enhanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff314e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = 'logistic-regression-enhanced'\n",
    "y_pred_probas = get_lr_preds(X_enhanced[train_nid], y_enhanced[train_nid], X_enhanced[test_nid], seed=23)\n",
    "\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y_enhanced[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GBoost-enhanced'\n",
    "y_pred_probas = get_gb_preds(X_enhanced[train_nid], y_enhanced[train_nid], X_enhanced[test_nid], seed=23)\n",
    "\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y_enhanced[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6f34c",
   "metadata": {},
   "source": [
    "It looks like in this case the new added features from the graph are not achieving a better performance for the Machine Learning model, this could be due to the model already doind a pretty good job with the non-graph features and because of the social graph not provding usefull information for fraude detection that makes sense given that fraudulents probably would try to hide from connections. Maybe another type of graph can be better to this task.\n",
    "\n",
    "Given the rarity of these Graph-Based Features they carry vastly different information from the tipically used features and as such allow the model to better differentiate classes, that can lead to better performance in models.This conclusions is further developed on our previously published paper: Supporting Financial Inclusion with Graph Machine Learning and Super-App Alternative Data; where we prove how graph based features augments the AUC of Credit RIsk models up to 4-5 percentage points!\n",
    "\n",
    "https://arxiv.org/abs/2102.09974\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c61ce",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/paper_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea04877",
   "metadata": {},
   "source": [
    "The above Figure is taken from the paper, there, the authors show how Graph-Based Features Enhanced models improve the results in terms of predicting credit default over non-Grap-based features models (Base in the figure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10660e9b",
   "metadata": {},
   "source": [
    "## Transductive vs Inductive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fc6e0",
   "metadata": {},
   "source": [
    "As shown in the paper and in the simple example above, the enhancing of traditional ML pipelines with graph-based features can be highly benefitial for the performance of the model. The question then is why don't we stop at this point?. What's the point of developing highly complex algortihms such as Graph Neural Networks?\n",
    "\n",
    "This serves us as an introduction to the next topic, Transductive & Inductive Learning.\n",
    "\n",
    "Transductive Learning is when the model learns from the complete graph, and through the hiding of some labels is tests on some masked subset of nodes. This means that when a set of unseen nodes is added the model can't handle those new nodes and has to be retrained.\n",
    "\n",
    "Inductive Learning is when the model desn't need to have access to al nodes and edges of the graph for training, as such we can train with a section of the graph and test on the other previously unseen section. Thos also means that the model can be trained on a graph or a graph in a period of time, and then use that same model for another similar graph or for new node and edge additions through time.\n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/transductive_inductive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc80f7",
   "metadata": {},
   "source": [
    "#### Now let's take a few minutes to think whether the previously built model can be used in a Transductive or Inductive setting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186e4ab",
   "metadata": {},
   "source": [
    "Hint: Use the definitions of the Graph Based Features added to get to this answer:\n",
    "\n",
    "PageRank: https://es.wikipedia.org/wiki/PageRank\n",
    "Degree: https://en.wikipedia.org/wiki/Degree_(graph_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68272f",
   "metadata": {},
   "source": [
    "Your answer goes here:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c0744",
   "metadata": {},
   "source": [
    "#### The Inductive nature of Graph Neural Networks is the reason why they have taken so much popularity. With them we do not need to have expert knowledge crafting of graph based features, and we can also train a model that can then be used in other similar graphs or in evolutions of the graph where it was trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af129482",
   "metadata": {},
   "source": [
    "## Graph Neural Networks - Introduction to DGL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece4088",
   "metadata": {},
   "source": [
    "Let's start by giving a brief introduction to DGL.\n",
    "\n",
    "DGL, acronym for Deep Graph Library, is an easy to use library for the creation of deep learning models on graphs (also known as Graph Neural Networks or GNN's). This package is framework agnostic, being able to use PyTorch, TensorFlow or Apache MXNet. It's also efficient and scalable using fast and memory efficient message passing primitive for training GNN's that can be scaled using GPU acceleration.\n",
    "\n",
    "Here you can find more information on the package or educative examples where it's used: https://www.dgl.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be67762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8480bf",
   "metadata": {},
   "source": [
    "As said on the presentation, there are many taks that can be performed with GNN's:\n",
    "\n",
    "1. Node Classification: Where we want to predict whether a node belong to a certain class, think of the previous task where we wanted to determine whether users were fraudulent.\n",
    "2. Link Prediction: Where we want to predict the likeliness of every pair of nodes being connected. Think of a reccomender system of friend in Facebook, the likelier a connection is the more would the platform reccomend you add that person.\n",
    "3. Graph Classification: Where we want to perfeor classification on a certain structure of nodes or in a whole graph, this can be used for families detection in a graph.\n",
    "\n",
    "For the purpose of this workshop and considering time-constraint we will be reviewing Node Classification Tasks. Performing the same classification task done with the previous traditional ML models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010076b3",
   "metadata": {},
   "source": [
    "#### Let's first remember a few things from the graph, do you think we are dealing with a heterogeneous or homogeneous graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4df14",
   "metadata": {},
   "source": [
    "That is quite relevant for the GNN model to be used because it dictates what kind of models we can use. For example Graph Convolutional Network (GCN) can only handle homogeneous graphs, if we have a heterogenous graph we have to use a variant in it's implementation called Relational Graph Convolutional Networks (RGCN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d66cdc",
   "metadata": {},
   "source": [
    "#### GCN Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb44a3d",
   "metadata": {},
   "source": [
    "This model was the first GNN developed, it is considered as one of the most basic GNN variants. This model was developed by Kipf and Welling, published in 2017 with this paper: https://arxiv.org/pdf/1609.02907.pdf.\n",
    "\n",
    "The convolution concept used here is quite similar to that of the Convolutional Neural Networks (CNN) tipically used in Computer Vision. The key difference here is that a CNN is built for Euclidean structured data, and graphs are definitely not that. \n",
    "\n",
    "This can be easily explained with this example, in images, when we separate those on pixels for Computer Vision we know that every pixel has the same amount of pixels surrounding it, they follow an specific order it's a perfect grid structure. In graphs, not every node has the same amount of neighbors and there is no intrinsic order in them, those irregularities give graphs their non-Euclidean structure nature.\n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/convolutions.png)\n",
    "\n",
    "source: https://arxiv.org/pdf/1901.00596.pdf\n",
    "\n",
    "Therefore, most GNN's are modifications to take into account the non-Euclidean nature of graphs. Mostly, this is done via permutation invariant aggregations (like the sum or average) of the neighbors for each node, this is commonly known as message passing.\n",
    "\n",
    "In the case of GCN the formula for the forward of the layers is the following:\n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/gcn.png)\n",
    "\n",
    "Here, through the dot product between A & H we are transforming the representation from their features or previous representationto the sum of the neighboring nodes features or representations, to then be passed through a selected non-linearity function represented by sigma (This can be a Relu, Leaky Relu, sigmoid, etc). We need to take into account that this does not aggregate the nodes own features unless there are self-loops for every node, this is a common modification done to graphs for GNN's.\n",
    "\n",
    "It's also important to notice that with each forward message passing of the GNN we are aggregating another hope aways of neighbors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2f10f",
   "metadata": {},
   "source": [
    "#### Now let's code that up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba268",
   "metadata": {},
   "source": [
    "Let's build a two layer GCN that classfies our nodes on whether they are fraudsters or not! With DGL this is as easy as creating a GCN model class that calls the method GraphConv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN Class\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        #Here we go from our original number of features to the size of our hidden representations\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        #Here we go from the hidden representation to the dimension of the number of classes for the probabilities\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat) #Here we apply the first convolution\n",
    "        h = F.relu(h) #Here we apply the selected non-linearity. In this case a Relu\n",
    "        h = self.conv2(g, h) #Here we apply the second convolution\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3477d26",
   "metadata": {},
   "source": [
    "Then we can simply create the model with the given attributes, we will be using the feature set without the Graph-Based features here and a hidden state size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "num_classes = 2\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(X.shape[1], hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d8550",
   "metadata": {},
   "source": [
    "And now we will create the training function using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, features, labels, train_mask, test_mask, epochs, model):\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=0.01) #Selected optimizer\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    #Here we create the validation set with a portion of the train set\n",
    "    val_mask = train_mask[:len(train_mask) // 5]\n",
    "    train_mask = train_mask[len(train_mask) // 5:]\n",
    "\n",
    "    train_mask = train_mask.nonzero().squeeze()\n",
    "    test_mask = test_mask.nonzero().squeeze()\n",
    "    val_mask = val_mask.nonzero().squeeze()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "    \n",
    "    #Check results on test set\n",
    "    y_pred_probas = model(g, features).detach().numpy()\n",
    "    y_pred_probas = y_pred_probas[test_mask]\n",
    "    y_preds = y_pred_probas[:,1]\n",
    "    model_name = 'GCN'\n",
    "    \n",
    "    auc, f1, prec, recall = get_results(y_pred_probas, labels[test_nid], 0.5)\n",
    "    dict_results = {'Model':model_name, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "    results_df = pd.DataFrame(columns=['Model','AUC','F1 Score','Precision','Recall'])\n",
    "    results_df = results_df.append(dict_results, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334270e",
   "metadata": {},
   "source": [
    "After that, we have to create the dgl graph to send into the GNN's. It is not the same as in NetworkX this is not a hard process either as you shall see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by using the same edges df but transforming the from and to columns into arrays\n",
    "src = edges_df['~from'].to_numpy()\n",
    "snk = edges_df['~to'].to_numpy()\n",
    "\n",
    "G_dgl = dgl.graph((src,snk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bd599",
   "metadata": {},
   "source": [
    "Additionally, DGL graphs objects also have some cool methods in it like a fast description of the whole graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f5b0d",
   "metadata": {},
   "source": [
    "That way we can quickly check the number of nodes or edges. Additionally, when dealing with heterogeneous graphs we can also see the different type of both nodes and edges. The n_data and e_data schemes are there if we want to directly add attributes to either the nodes or edges of the graph respectively.\n",
    "\n",
    "More information about what we can do with DGL graph objects can be found here: https://docs.dgl.ai/en/0.6.x/generated/dgl.graph.html\n",
    "\n",
    "We also have to add self-loops to the Graph so that the DGL model also takes the nodes own information in the aggregation. Let's do that using the simple add_self_loop() method of DGL (https://docs.dgl.ai/en/0.6.x/generated/dgl.add_self_loop.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dgl.add_edges(G_dgl.nodes(), G_dgl.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e58047",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "results_df = train(G_dgl, X, y, train_mask, test_mask, epochs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a84382",
   "metadata": {},
   "source": [
    "As you can see the results are not up to par with the other models, although this model hasn't been optimized at all in terms of hyperparameters. That could up the performance to better levels. Additionally these models are quite usefull because there is no necesarry construction of new features or expert inputs in the crafting of the model. This along with the inductive nature of the model allows us to use it on any similar graph!\n",
    "\n",
    "We have also proven the effectiveness of GNN's (More specifically RGCN) on more complex datasets and graphs in our paper: https://arxiv.org/abs/2107.13673\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130e9c9",
   "metadata": {},
   "source": [
    "#### As said before there are many other GNN models, for this specific graph we can use more complex variants like GAT, GraphSage or GIN. Those achieve better generalization of the task at hand and usually lead to better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40fac3",
   "metadata": {},
   "source": [
    "Other GNN's that we recommend trying for this same dataset changing the model calss object:\n",
    "\n",
    "GAT introduces attentions to the GCN model, weighting neighbors with certain features more for the classification task. More information can be found here: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html\n",
    "\n",
    "GraphSAGE is a general inductive framework that leverages node feature information (e.g.,  text  attributes) to  efficiently  generate  node  embeddings  forpreviously unseen data. More information can be found here: https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage\n",
    "\n",
    "GIN is the most expressive GNN developed to this date. More information can be found here: https://github.com/dmlc/dgl/tree/master/examples/pytorch/gin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023c11c",
   "metadata": {},
   "source": [
    "Now you can try to modify the model class object with whichever other model you prefer and compare the results with GCN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614f0fc",
   "metadata": {},
   "source": [
    "We hope you liked this introduction to GNN's this is a highly compeling subject that is currently on of the most researched on the world. Feel free to contact us to any questions you may have about this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "af92ff48b314d6de65d7d307803e052e27c55ee0db97487118890f19379fef9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
