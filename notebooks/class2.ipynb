{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6522dbfa",
   "metadata": {},
   "source": [
    "# Graph Machine Learning Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2efa87",
   "metadata": {},
   "source": [
    "by Alejandro Correa Bahnsen, Jaime D. Acevedo-Viloria & Luisa Roa\n",
    "\n",
    "version 1.2, October 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1cac9",
   "metadata": {},
   "source": [
    "In this notebook we will be doing a brief introduction to graph machine learning. The agenda is as follows:\n",
    "\n",
    "\n",
    "1. Different types of graphs - Introduction to NetworkX\n",
    "2. Creating Graph Based Features and enhancing ML models\n",
    "3. Creating a Graph from own data using NetworkX\n",
    "4. Transductive Learning vs. Inductive Learning\n",
    "5. Graph Neural Networks - Introduction to DGL\n",
    "\n",
    "Through these basics you will be able to leverage graphs for the enhacement of Machine Learning models, or be able to learn the basics of how to build Neural Networks specially crafted for Graphs. We hope you like it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install libraries\n",
    "!pip install dgl==0.6.1\n",
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eb36d",
   "metadata": {},
   "source": [
    "## Types of Graphs - An Introduction to NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee69b25",
   "metadata": {},
   "source": [
    "NetworkX according to it's creators is: NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. \n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/networkx_description.png)\n",
    "\n",
    "https://networkx.org/\n",
    "\n",
    "We will be using the latest stable 2.8.7 version of the Package as referenced on the requirements .txt provided in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8343044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd87c3",
   "metadata": {},
   "source": [
    "Let's start by describing different properties graphs can have and what those mean for the graph in subject. We will use NetworkX visual examples for every one of them and we will also describe real world applications where you may find such type of graph.\n",
    "\n",
    "We will be using different NetworkX to innitialize graphs, this is just to highlight the many different ways we can do this. Make sure to check the documentation for more info in this: https://networkx.org/documentation/stable/reference/generators.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af8b14",
   "metadata": {},
   "source": [
    "### Directed & Undirected Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e856bd",
   "metadata": {},
   "source": [
    "This property refers as to whether the edges connecting the graphs have an inherent direction in it.\n",
    "\n",
    "In undirected the graphs, edges indicate a two-way relationship, and as such they can be traversed from either node to other connected. In directed graphs, edges indicate a one-way direction. Meaning that they can only be traversed in an specific direction of the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilize a random graph\n",
    "g_gaussian = nx.gaussian_random_partition_graph(25, 4, 5, 0.25, 0.1, directed=False)\n",
    "\n",
    "#Draw the graph structure\n",
    "nx.draw(g_gaussian, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a random directed graph\n",
    "g_gaussian_dir = nx.gaussian_random_partition_graph(25, 4, 5, 0.25, 0.1, directed=True, seed=23)\n",
    "\n",
    "#Draw the graph structure\n",
    "nx.draw(g_gaussian_dir, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e2bd5",
   "metadata": {},
   "source": [
    "This is normally described by the use of an arrow pointing the direction of the edge, if there is no arrow then we can assume the graph is undirected. We can see this in the above networkX examples, where we create both a undirected and directed Random Gaussian Graph setting the directed parameter to True and False respectively.\n",
    "\n",
    "This is a very important characteristic in real life application. When creating graphs to describe real-life processes we might have an Instagram like Social Network, where an user can follow another user but it doesn't have to be the other way around. This would be a directed graph.\n",
    "\n",
    "We may also have a Facebook like Social Network, where when a friend request is sent and accepted both users are instantly connected to each other. This could be described by an undirected graph.\n",
    "\n",
    "We can also see this, when extracting the edges of the graph with the edges() method, take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35acb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_gaussian.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_gaussian_dir.edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e00f86",
   "metadata": {},
   "source": [
    "This accesses the EdgeView for the undirected graphs where all edges are given as undirected. While, on the directed graph we can see that it defaults to the OutEdges of the graph, meaning that those tuples represent just a one-way path from one node to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e2677",
   "metadata": {},
   "source": [
    "### Weighted and Unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ac259",
   "metadata": {},
   "source": [
    "Another property that graphs can have is that they have edges with different weights, this means that in the relationship between two nodes there is a numerical value that indicates the magnitude of how strong or important the relationship is. In this way, weighted graphs are those that contain a weight on each edge while a unweighted graph does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9afc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Inicialize a random graph\n",
    "G=nx.gnm_random_graph(35, 50, seed=12)\n",
    "\n",
    "#Assign a random weights\n",
    "for (u, v) in G.edges():\n",
    "    G.edges[u,v]['weight'] = random.random()\n",
    "\n",
    "#Define type of relationship\n",
    "family = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] >= 0.6]\n",
    "friend = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] < 0.6]\n",
    "\n",
    "#Plot\n",
    "pos = nx.spring_layout(G, seed=124)\n",
    "nx.draw_networkx_nodes(G, pos)\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos, edgelist=family, width=2, edge_color='r', alpha = 0.4\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos, edgelist=friend, width=1, edge_color='b', alpha = 0.4\n",
    ")\n",
    "\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f0cf2",
   "metadata": {},
   "source": [
    "We create a graph with 35 nodes and 50 random edges with the gnm_random_graph function, then we assign the property 'weights' to the edges with a random probability. Lets now assume that it is a social network, where each node is a person who has different connections with friends or family, if the weight is greater than 0.6 then there is a closer connection so they are family, but if the weight is less than 0.6 then the relationship is less strong and they are friends.\n",
    "\n",
    "Although this is a simple example, in many real-world applications it is an important characteristic to consider since not all relatinoships are equal. Furthermore, different graphs algorithms are affected, for example in the shortest path algorith what used to be the shortest path can also be the most expensive given the weights in the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4de243",
   "metadata": {},
   "source": [
    "### Sparse & Dense Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e283a",
   "metadata": {},
   "source": [
    "This is an interesting distinction, the density of the graph refers to both the number of nodes and how connected are them. In mathematics, a dense graph is a graph in which the number of edges is close to the maximal number of edges. The maximal number of edges being all the pair combination of nodes in an undirected graph, and all the pair permutation of nodes in an undirected graph.\n",
    "\n",
    "In other words, the more close to fully connected the graph is - fully connected meaning that every node is connected to each other-, the more dense we can say the graph is. If the graph is a bunch of disjunct islands of nodes, we can safely say that is a sparse graph. Let's see some networkX examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c60df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of nodes n\n",
    "n = 25\n",
    "\n",
    "#Initialize the sparse graph\n",
    "g_sparse = nx.erdos_renyi_graph(n, 0.08,seed=23)\n",
    "\n",
    "#Draw the graph structure\n",
    "nx.draw(g_sparse, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8063f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the dense graph\n",
    "g_dense = nx.erdos_renyi_graph(n, 0.15,seed=23)\n",
    "\n",
    "#Draw the graph structure\n",
    "nx.draw(g_dense, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8cdc6",
   "metadata": {},
   "source": [
    "We create the graphs using the generator method erdos_renyi_graph, this is a commonly used model to generate random graphs from two parameters: The number of nodes n, and the probability to create an edge for every pair of nodes p.\n",
    "\n",
    "Therefore, for the sparse graph we set up a small probability of 0.08, and for the dense graph we set up a probability of 0.15.\n",
    "\n",
    "We can also measure the Density of the graph, as the ratio of the number of edges with respect to the maximal number of edges. Therefore we can use the following formulas for undirected and directed graphs:\n",
    "\n",
    "Undirected: ![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/undirected.png)\n",
    "\n",
    "Directed: ![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/directed.png)\n",
    "\n",
    "Now, let's measure the density of the previously created graphs!\n",
    "\n",
    "First we need the number of edges for both graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_sparse = g_sparse.number_of_edges()\n",
    "e_dense = g_dense.number_of_edges()\n",
    "\n",
    "print(\"The number of edges for the sparse graph is: \", e_sparse)\n",
    "print(\"The number of edges for the dense graph is: \", e_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9065fc",
   "metadata": {},
   "source": [
    "As you can see we used the number_of_edges() method to measure how many edges we have in each graph automatically. Here we can see other properties we can take from NetworkX graphs: https://networkx.org/documentation/stable/reference/classes/graph.html.\n",
    "\n",
    "Like for example, wether a certain node or edge exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629d727",
   "metadata": {},
   "source": [
    "Now try to recreate the formula for undirected graphs in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95017b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_undirected_graph_density(g):\n",
    "    '''\n",
    "    Hint you should use the Graph Class attributes to get\n",
    "    the number of nodes and Edges\n",
    "    '''\n",
    "    n_nodes = 0 #Your code goes here instead of the zero\n",
    "    n_edges = 0 #Your code goes here instead of the zero\n",
    "    density = 0 #Your code goes here instead of the zero\n",
    "    \n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8946744",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_sparse = calculate_undirected_graph_density(g_sparse)\n",
    "density_dense = calculate_undirected_graph_density(g_dense)\n",
    "\n",
    "print(\"The density of the sparse graph is: \",round(density_sparse,2))\n",
    "print(\"The density of the dense graph is: \",round(density_dense,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64595d70",
   "metadata": {},
   "source": [
    "Now let's try the directed graph density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_directed_graph_density(g):\n",
    "    '''\n",
    "    Hint you should use the Graph Class attributes to get\n",
    "    the number of nodes and Edges\n",
    "    '''\n",
    "    n_nodes = 0 #Your code goes here instead of the zero\n",
    "    n_edges = 0 #Your code goes here instead of the zero\n",
    "    density = 0 #Your code goes here instead of the zero\n",
    "    \n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_directed = calculate_directed_graph_density(g_gaussian_dir)\n",
    "\n",
    "print(\"The density of the directed graph is: \",round(density_directed,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab94e0",
   "metadata": {},
   "source": [
    "In real-life, determining whether a graph is dense or sparse is quite subjective to the problem at hand. For example, we can use this metric to compare two different social networks and then compare one with each other in terms of density. Another highly used strategy to effectively measure the density of a given graph is via the comparison to multiple generated random graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b3938",
   "metadata": {},
   "source": [
    "### Homogeneous and Heterogeneous Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb7eaf",
   "metadata": {},
   "source": [
    "Last, but definitely not least, we have Homogeneous and Heterogeneous Graphs. This distinction goes into the types of both the nodes and the edges in the graph. In cases where we have only one type of node and only one type of relationship we are dealing with an Homogeneous Graph, any othe type of node or edge added would then be a Heterogeneous Graph.\n",
    "\n",
    "This is a really important distinction, because those graph are inmensely different both in their complexity -where Homogeneous Graphs tend to be much simpler-, and in the Machine Learning methodologies we can use to deal with them that we will study in later posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e012da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Inicialize a random graph\n",
    "G=nx.erdos_renyi_graph(n, 0.15,seed=45)\n",
    "\n",
    "#Assign type of node\n",
    "for u in G.nodes():\n",
    "    rand = random.randint(0, 48)\n",
    "    G.nodes[u]['Type'] = 1 if rand < 12 else 0\n",
    "\n",
    "#Draw the graph with specified colors for each node type\n",
    "color_val = [nx.get_node_attributes(G,'Type').get(node) for node in G.nodes()]\n",
    "nx.draw(G, pos, with_labels=True, node_color = color_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09947f7a",
   "metadata": {},
   "source": [
    "In this case, we construct a random graph with 25 nodes and a probability of 0.15 of having an edge between nodes. For our example, we consider a setting of a communications company where the graph considers calls between users. Particularly, the company wants to differentiate between new users (less than 12 months) and old users (more than 12 months) to understand if there are different behaviors between users.\n",
    "\n",
    "As in our example, most real-life graphs are heterogeneous graphs where different entities interact. It is quite complicated to describe a process with only one type of entity interacting with other entities all of the same type, making Heterogenous Graphs more challenging to deal with but also more expressive of the process they describe.\n",
    "\n",
    "It's also relevant to note that different type of nodes may have different characteristics or features. Think of a paper-author network, where we have author relationships between Author and Paper nodes, and citation relationships where Paper nodes connect with each other if they have cited any other paper. In this type of example, we may have really distinct feature sets between the Authors and the Papers, and as such we have to be more creative when dealing with those networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2719c28",
   "metadata": {},
   "source": [
    "NetworkX also has a number of pre-determined made Graphs, such as this one with the coappearance network of characters in the novel Les Miserables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_graph = nx.les_miserables_graph()\n",
    "nx.draw(lm_graph, with_labels=True)\n",
    "\n",
    "characters = lm_graph.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d110e9",
   "metadata": {},
   "source": [
    "Now let's try to get the neighbors of the character Myriel using a built-in function of the NetworkX Graph Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint you should transform the output of the built-in function into a list to show the results\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31216a",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "With any of the Graph Generators (https://networkx.org/documentation/stable/reference/generators.html) Create a graph of preference, and through a built-in method of the NetworkX Graph Class get the neighbors of a certain node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code Goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4a7bd",
   "metadata": {},
   "source": [
    "## Creating Graph Based Features and enhancing ML models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4772b",
   "metadata": {},
   "source": [
    "Now let's see how we can use these new features taken from the graph to enhance our ML models. First, let's import the information of the users in the graph from the nodes_features_workshop csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/jdacevedo3010/graph-mahine-learning-workshop/raw/master/data/nodes_features_workshop.csv').set_index('USER_ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b41975",
   "metadata": {},
   "source": [
    "Here we have a DataFrame with the users as the index. The columns contain the features that profile them:\n",
    "\n",
    "1. Device Type: An encoding of the different devices in the dataset\n",
    "2. Expected Value: A score that measures the value the client will bring\n",
    "3. Sales: Total amount spent by the user\n",
    "\n",
    "And a label that tells us whether the user is fraudulent or not (FRAUD column).\n",
    "\n",
    "Let's use this information to train a couple of traditional Machine Learning models such as: Gradient Boosting Trees, and Logistic Regression. Let's first create train and test masks, we will do this manually given that we wil later need this same division for the Graph Neural Networks. We will also be using torch tensors for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d266d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def create_masks(df, seed=23, test_size=0.2):\n",
    "    '''\n",
    "    This function creates binary tensors that indicate whether an user is on the train or test set\n",
    "    '''\n",
    "    np.random.RandomState(seed)\n",
    "    temp_df = df.copy()\n",
    "    temp_df['split_flag'] = np.random.random(df.shape[0])\n",
    "    train_mask = th.BoolTensor(np.where(temp_df['split_flag'] <= (1 - test_size), True, False))\n",
    "    test_mask = th.BoolTensor(np.where((1 - test_size) < temp_df['split_flag'] , True, False))\n",
    "    return train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary masks\n",
    "train_mask, test_mask = create_masks(df, 23, 0.3)\n",
    "\n",
    "print(train_mask)\n",
    "\n",
    "#Here we transform the tensors so they indicate the indices of the train and test users instead of the binary\n",
    "train_nid = train_mask.nonzero().squeeze()\n",
    "test_nid = test_mask.nonzero().squeeze()\n",
    "\n",
    "print(train_nid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4c6d0",
   "metadata": {},
   "source": [
    "Now, let's create the X and Y tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and Y dataframes\n",
    "X = df.drop(['FRAUD'], axis=1)\n",
    "y = df.drop(['DEVICE_TYPE','EXPECTED_VALUE','SALES'], axis=1)\n",
    "\n",
    "print('The shape of the X DataFrame is: ',X.shape)\n",
    "print('The shape of the y DataFrame is: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ff17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the X and Y dataframes to tensors now as well\n",
    "X = th.tensor(X.values).float()\n",
    "y = th.tensor(y.values).type(th.LongTensor).squeeze_()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d17495",
   "metadata": {},
   "source": [
    "Let's create the functions to train the ML models, and a function that allows us to measure the performance of those models in terms of ROC Curve AUC, F1-Score, Precision and Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def get_gb_preds(X_train, y_train, X_test, seed=23):\n",
    "    clf = GradientBoostingClassifier(random_state=seed)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred_probas = clf.predict_proba(X_test)\n",
    "    return y_pred_probas\n",
    "\n",
    "def get_lr_preds(X_train, y_train, X_test, seed=23):\n",
    "    clf = LogisticRegression(random_state=seed)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred_probas = clf.predict_proba(X_test)\n",
    "    return y_pred_probas\n",
    "\n",
    "def get_results(y_pred_probas, y_test, threshold=0.5):\n",
    "    pred_probas_1 = y_pred_probas[:,1]\n",
    "    preds_1 = np.where(pred_probas_1>threshold,1,0)\n",
    "    auc = roc_auc_score(y_test, pred_probas_1)\n",
    "    f1 = f1_score(y_test,preds_1)\n",
    "    prec = precision_score(y_test,preds_1)\n",
    "    recall = recall_score(y_test,preds_1)\n",
    "    return auc, f1, prec, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e94bca",
   "metadata": {},
   "source": [
    "#### Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043be43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[train_nid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'logistic-regression'\n",
    "y_pred_probas = get_lr_preds(X[train_nid], y[train_nid], X[test_nid], seed=23)\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Model','AUC','F1 Score','Precision','Recall'])\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623b31b",
   "metadata": {},
   "source": [
    "#### GBoost results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GBoost'\n",
    "y_pred_probas = get_gb_preds(X[train_nid], y[train_nid], X[test_nid], seed=23)\n",
    "\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138f6f3",
   "metadata": {},
   "source": [
    "## Creating a Graph from own data using NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a7b8d",
   "metadata": {},
   "source": [
    "We will know see how to create a graph from data instead of randomly. For this we will have to import the csv's in the data folder and process them for NetworkX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a961e",
   "metadata": {},
   "source": [
    "First lets import the relations csv in the data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "edges_df = pd.read_csv('https://github.com/jdacevedo3010/graph-mahine-learning-workshop/raw/master/data/new_edges_workshop.csv')\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53582be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20e03b",
   "metadata": {},
   "source": [
    "Here you a 2-colummn DataFrame that contains the undirected edges between distinct users. This is normally referred to as \"List of edges\" and it's a common way to create graphs. NetworkX also has a method to create a graph from a DataFrame of edges, let's do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf4f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edges_df,'~from','~to')\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27856d06",
   "metadata": {},
   "source": [
    "Let's draw a portion of the graph to check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_draw = nx.from_pandas_edgelist(edges_df.head(100),'~from','~to')\n",
    "nx.draw(G_draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6165be",
   "metadata": {},
   "source": [
    "Nice! We can now use this created graph to extract characteristics from it. \n",
    "\n",
    "Let's say we want to get the number of neighbors of each node in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G.degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401329e",
   "metadata": {},
   "source": [
    "We can use the degree method in NetworkX to create a dictionary that holds the node id's as the keys and the degree of that node as the value. There are also plenty of other measures, we can extract from the Graph object of NetworkX like centrality or betweeness metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1237cc",
   "metadata": {},
   "source": [
    "More information about the NetworkX library can be found in this tutorial: https://networkx.org/nx-guides/content/tutorial.html. Or in the overall documentation guide of the package: https://networkx.org/nx-guides/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02facc",
   "metadata": {},
   "source": [
    "#### And now let's enhance the features with some extracted from the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcecb90",
   "metadata": {},
   "source": [
    "Now we will use the previously generated dictionary of degrees as an additional feature to the DataFrame, along with the centrality measure PageRank.\n",
    "\n",
    "PageRank was developed by Google and measures the importance of a node in a Graph given how connected are the node's neighbors. More information can be found here: https://es.wikipedia.org/wiki/PageRank\n",
    "\n",
    "Let's first calculate that for the previously generated graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ba92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(G,alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f1b40",
   "metadata": {},
   "source": [
    "Let's take a look at why those values for two nodes are so different. First, we will select a high PageRank node and a low PageRank node. Then, we need to get the 2-hop neighbors of those nodes in a list using the neighbors() method (https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.neighbors.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The PageRank of the selected highly central node is: ',pr[38709])\n",
    "print('The PageRank of the selected lowly central node is: ',pr[125388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69911eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_2d_list(matrix):\n",
    "    flatten_matrix = []\n",
    "    for sublist in matrix:\n",
    "        for val in sublist:\n",
    "            flatten_matrix.append(val)\n",
    "    return flatten_matrix\n",
    "\n",
    "high_neighbors = [n for n in G.neighbors(38709)]\n",
    "high_neighbors_2 = flatten_2d_list([[n for n in G.neighbors(n2)] for n2 in high_neighbors])\n",
    "low_neighbors = [n for n in G.neighbors(125388)]\n",
    "low_neighbors_2 = flatten_2d_list([[n for n in G.neighbors(n2)] for n2 in low_neighbors])\n",
    "\n",
    "\n",
    "print(\"Number of 2-hop neighbors of high PageRank: \",len(high_neighbors_2))\n",
    "print(\"Number of 2-hop neighbors of low PageRank: \", len(low_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f73a7",
   "metadata": {},
   "source": [
    "As mentioned before, the PageRank measures the centrality of a node given the connections of his neighbors. So in a social network, if my friends have many friends then I'll be a highly central node.\n",
    "\n",
    "Now let's draw those subgraphs using the subgraph method of networkx (https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.subgraph.html) for a better look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G.subgraph(high_neighbors_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G.subgraph(low_neighbors_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943e9be",
   "metadata": {},
   "source": [
    "We can see that the highly central node has every node highly interconnected with each other, while the low PageRank node has a cluster in the middle and the a satelite circunference around that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36223be7",
   "metadata": {},
   "source": [
    "And now let's add both the degree and PageRank as features for the users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map Degree and PageRank into the DataFrame\n",
    "df_enhanced = df.copy()\n",
    "df_enhanced['DEGREE'] = df.index.map(degrees)\n",
    "df_enhanced['PAGERANK'] = df.index.map(pr)\n",
    "\n",
    "df_enhanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14256e",
   "metadata": {},
   "source": [
    "And finally, let's run the same models as before with these new features to see how the results compare with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and Y dataframes\n",
    "X_enhanced = df_enhanced.drop(['FRAUD'], axis=1).fillna(0)\n",
    "y_enhanced = df_enhanced.drop(['DEVICE_TYPE','EXPECTED_VALUE','SALES','DEGREE','PAGERANK'], axis=1)\n",
    "\n",
    "print('The shape of the X DataFrame is: ',X.shape)\n",
    "print('The shape of the y DataFrame is: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6734b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_enhanced[['DEGREE','PAGERANK']] = scaler.fit_transform(X_enhanced[['DEGREE','PAGERANK']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.tensor(X_enhanced.values).float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11456462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the X and Y dataframes to tensors now as well\n",
    "X_enhanced = th.tensor(X_enhanced.values).float()\n",
    "y_enhanced = th.tensor(y_enhanced.values).type(th.LongTensor).squeeze_()\n",
    "\n",
    "print(X_enhanced.shape)\n",
    "print(y_enhanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff314e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = 'logistic-regression-enhanced'\n",
    "y_pred_probas = get_lr_preds(X_enhanced[train_nid], y_enhanced[train_nid], X_enhanced[test_nid], seed=23)\n",
    "\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y_enhanced[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GBoost-enhanced'\n",
    "y_pred_probas = get_gb_preds(X_enhanced[train_nid], y_enhanced[train_nid], X_enhanced[test_nid], seed=23)\n",
    "\n",
    "auc, f1, prec, recall = get_results(y_pred_probas, y_enhanced[test_nid], 0.5)\n",
    "dict_results = {'Model':model, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "results_df = results_df.append(dict_results, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6f34c",
   "metadata": {},
   "source": [
    "It looks like in this case the new added features from the graph are not achieving a better performance for the Machine Learning model, this could be due to the model already doind a pretty good job with the non-graph features and because of the social graph not provding usefull information for fraude detection that makes sense given that fraudulents probably would try to hide from connections. Maybe another type of graph can be better to this task.\n",
    "\n",
    "Given the rarity of these Graph-Based Features they carry vastly different information from the tipically used features and as such allow the model to better differentiate classes, that can lead to better performance in models.This conclusions is further developed on our previously published paper: Supporting Financial Inclusion with Graph Machine Learning and Super-App Alternative Data; where we prove how graph based features augments the AUC of Credit RIsk models up to 4-5 percentage points!\n",
    "\n",
    "https://arxiv.org/abs/2102.09974\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c61ce",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/paper_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea04877",
   "metadata": {},
   "source": [
    "The above Figure is taken from the paper, there, the authors show how Graph-Based Features Enhanced models improve the results in terms of predicting credit default over non-Grap-based features models (Base in the figure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10660e9b",
   "metadata": {},
   "source": [
    "## Transductive vs Inductive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fc6e0",
   "metadata": {},
   "source": [
    "As shown in the paper and in the simple example above, the enhancing of traditional ML pipelines with graph-based features can be highly benefitial for the performance of the model. The question then is why don't we stop at this point?. What's the point of developing highly complex algortihms such as Graph Neural Networks?\n",
    "\n",
    "This serves us as an introduction to the next topic, Transductive & Inductive Learning.\n",
    "\n",
    "Transductive Learning is when the model learns from the complete graph, and through the hiding of some labels is tests on some masked subset of nodes. This means that when a set of unseen nodes is added the model can't handle those new nodes and has to be retrained.\n",
    "\n",
    "Inductive Learning is when the model desn't need to have access to al nodes and edges of the graph for training, as such we can train with a section of the graph and test on the other previously unseen section. Thos also means that the model can be trained on a graph or a graph in a period of time, and then use that same model for another similar graph or for new node and edge additions through time.\n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/transductive_inductive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc80f7",
   "metadata": {},
   "source": [
    "#### Now let's take a few minutes to think whether the previously built model can be used in a Transductive or Inductive setting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186e4ab",
   "metadata": {},
   "source": [
    "Hint: Use the definitions of the Graph Based Features added to get to this answer:\n",
    "\n",
    "PageRank: https://es.wikipedia.org/wiki/PageRank\n",
    "Degree: https://en.wikipedia.org/wiki/Degree_(graph_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68272f",
   "metadata": {},
   "source": [
    "Your answer goes here:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c0744",
   "metadata": {},
   "source": [
    "#### The Inductive nature of Graph Neural Networks is the reason why they have taken so much popularity. With them we do not need to have expert knowledge crafting of graph based features, and we can also train a model that can then be used in other similar graphs or in evolutions of the graph where it was trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af129482",
   "metadata": {},
   "source": [
    "## Graph Neural Networks - Introduction to DGL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece4088",
   "metadata": {},
   "source": [
    "Let's start by giving a brief introduction to DGL.\n",
    "\n",
    "DGL, acronym for Deep Graph Library, is an easy to use library for the creation of deep learning models on graphs (also known as Graph Neural Networks or GNN's). This package is framework agnostic, being able to use PyTorch, TensorFlow or Apache MXNet. It's also efficient and scalable using fast and memory efficient message passing primitive for training GNN's that can be scaled using GPU acceleration.\n",
    "\n",
    "Here you can find more information on the package or educative examples where it's used: https://www.dgl.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be67762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8480bf",
   "metadata": {},
   "source": [
    "As said on the presentation, there are many taks that can be performed with GNN's:\n",
    "\n",
    "1. Node Classification: Where we want to predict whether a node belong to a certain class, think of the previous task where we wanted to determine whether users were fraudulent.\n",
    "2. Link Prediction: Where we want to predict the likeliness of every pair of nodes being connected. Think of a reccomender system of friend in Facebook, the likelier a connection is the more would the platform reccomend you add that person.\n",
    "3. Graph Classification: Where we want to perfeor classification on a certain structure of nodes or in a whole graph, this can be used for families detection in a graph.\n",
    "\n",
    "For the purpose of this workshop and considering time-constraint we will be reviewing Node Classification Tasks. Performing the same classification task done with the previous traditional ML models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010076b3",
   "metadata": {},
   "source": [
    "#### Let's first remember a few things from the graph, do you think we are dealing with a heterogeneous or homogeneous graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4df14",
   "metadata": {},
   "source": [
    "That is quite relevant for the GNN model to be used because it dictates what kind of models we can use. For example Graph Convolutional Network (GCN) can only handle homogeneous graphs, if we have a heterogenous graph we have to use a variant in it's implementation called Relational Graph Convolutional Networks (RGCN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d66cdc",
   "metadata": {},
   "source": [
    "#### GCN Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb44a3d",
   "metadata": {},
   "source": [
    "This model was the first GNN developed, it is considered as one of the most basic GNN variants. This model was developed by Kipf and Welling, published in 2017 with this paper: https://arxiv.org/pdf/1609.02907.pdf.\n",
    "\n",
    "The convolution concept used here is quite similar to that of the Convolutional Neural Networks (CNN) tipically used in Computer Vision. The key difference here is that a CNN is built for Euclidean structured data, and graphs are definitely not that. \n",
    "\n",
    "This can be easily explained with this example, in images, when we separate those on pixels for Computer Vision we know that every pixel has the same amount of pixels surrounding it, they follow an specific order it's a perfect grid structure. In graphs, not every node has the same amount of neighbors and there is no intrinsic order in them, those irregularities give graphs their non-Euclidean structure nature.\n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/convolutions.png)\n",
    "\n",
    "source: https://arxiv.org/pdf/1901.00596.pdf\n",
    "\n",
    "Therefore, most GNN's are modifications to take into account the non-Euclidean nature of graphs. Mostly, this is done via permutation invariant aggregations (like the sum or average) of the neighbors for each node, this is commonly known as message passing.\n",
    "\n",
    "In the case of GCN the formula for the forward of the layers is the following:\n",
    "\n",
    "![](https://raw.githubusercontent.com/jdacevedo3010/graph-mahine-learning-workshop/master/images/gcn.png)\n",
    "\n",
    "Here, through the dot product between A & H we are transforming the representation from their features or previous representationto the sum of the neighboring nodes features or representations, to then be passed through a selected non-linearity function represented by sigma (This can be a Relu, Leaky Relu, sigmoid, etc). We need to take into account that this does not aggregate the nodes own features unless there are self-loops for every node, this is a common modification done to graphs for GNN's.\n",
    "\n",
    "It's also important to notice that with each forward message passing of the GNN we are aggregating another hope aways of neighbors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2f10f",
   "metadata": {},
   "source": [
    "#### Now let's code that up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba268",
   "metadata": {},
   "source": [
    "Let's build a two layer GCN that classfies our nodes on whether they are fraudsters or not! With DGL this is as easy as creating a GCN model class that calls the method GraphConv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN Class\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        #Here we go from our original number of features to the size of our hidden representations\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        #Here we go from the hidden representation to the dimension of the number of classes for the probabilities\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat) #Here we apply the first convolution\n",
    "        h = F.relu(h) #Here we apply the selected non-linearity. In this case a Relu\n",
    "        h = self.conv2(g, h) #Here we apply the second convolution\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3477d26",
   "metadata": {},
   "source": [
    "Then we can simply create the model with the given attributes, we will be using the feature set without the Graph-Based features here and a hidden state size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "num_classes = 2\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(X.shape[1], hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d8550",
   "metadata": {},
   "source": [
    "And now we will create the training function using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, features, labels, train_mask, test_mask, epochs, model):\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=0.01) #Selected optimizer\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    #Here we create the validation set with a portion of the train set\n",
    "    val_mask = train_mask[:len(train_mask) // 5]\n",
    "    train_mask = train_mask[len(train_mask) // 5:]\n",
    "\n",
    "    train_mask = train_mask.nonzero().squeeze()\n",
    "    test_mask = test_mask.nonzero().squeeze()\n",
    "    val_mask = val_mask.nonzero().squeeze()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "    \n",
    "    #Check results on test set\n",
    "    y_pred_probas = model(g, features).detach().numpy()\n",
    "    y_pred_probas = y_pred_probas[test_mask]\n",
    "    y_preds = y_pred_probas[:,1]\n",
    "    model_name = 'GCN'\n",
    "    \n",
    "    auc, f1, prec, recall = get_results(y_pred_probas, labels[test_nid], 0.5)\n",
    "    dict_results = {'Model':model_name, 'AUC':auc, 'F1 Score':f1, 'Precision':prec, 'Recall':recall}\n",
    "    results_df = pd.DataFrame(columns=['Model','AUC','F1 Score','Precision','Recall'])\n",
    "    results_df = results_df.append(dict_results, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334270e",
   "metadata": {},
   "source": [
    "After that, we have to create the dgl graph to send into the GNN's. It is not the same as in NetworkX this is not a hard process either as you shall see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by using the same edges df but transforming the from and to columns into arrays\n",
    "src = edges_df['~from'].to_numpy()\n",
    "snk = edges_df['~to'].to_numpy()\n",
    "\n",
    "G_dgl = dgl.graph((src,snk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bd599",
   "metadata": {},
   "source": [
    "Additionally, DGL graphs objects also have some cool methods in it like a fast description of the whole graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f5b0d",
   "metadata": {},
   "source": [
    "That way we can quickly check the number of nodes or edges. Additionally, when dealing with heterogeneous graphs we can also see the different type of both nodes and edges. The n_data and e_data schemes are there if we want to directly add attributes to either the nodes or edges of the graph respectively.\n",
    "\n",
    "More information about what we can do with DGL graph objects can be found here: https://docs.dgl.ai/en/0.6.x/generated/dgl.graph.html\n",
    "\n",
    "We also have to add self-loops to the Graph so that the DGL model also takes the nodes own information in the aggregation. Let's do that using the simple add_self_loop() method of DGL (https://docs.dgl.ai/en/0.6.x/generated/dgl.add_self_loop.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dgl.add_edges(G_dgl.nodes(), G_dgl.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e58047",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "results_df = train(G_dgl, X, y, train_mask, test_mask, epochs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a84382",
   "metadata": {},
   "source": [
    "As you can see the results are not up to par with the other models, although this model hasn't been optimized at all in terms of hyperparameters. That could up the performance to better levels. Additionally these models are quite usefull because there is no necesarry construction of new features or expert inputs in the crafting of the model. This along with the inductive nature of the model allows us to use it on any similar graph!\n",
    "\n",
    "We have also proven the effectiveness of GNN's (More specifically RGCN) on more complex datasets and graphs in our paper: https://arxiv.org/abs/2107.13673\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130e9c9",
   "metadata": {},
   "source": [
    "#### As said before there are many other GNN models, for this specific graph we can use more complex variants like GAT, GraphSage or GIN. Those achieve better generalization of the task at hand and usually lead to better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40fac3",
   "metadata": {},
   "source": [
    "Other GNN's that we recommend trying for this same dataset changing the model calss object:\n",
    "\n",
    "GAT introduces attentions to the GCN model, weighting neighbors with certain features more for the classification task. More information can be found here: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html\n",
    "\n",
    "GraphSAGE is a general inductive framework that leverages node feature information (e.g.,  text  attributes) to  efficiently  generate  node  embeddings  forpreviously unseen data. More information can be found here: https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage\n",
    "\n",
    "GIN is the most expressive GNN developed to this date. More information can be found here: https://github.com/dmlc/dgl/tree/master/examples/pytorch/gin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023c11c",
   "metadata": {},
   "source": [
    "Now you can try to modify the model class object with whichever other model you prefer and compare the results with GCN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614f0fc",
   "metadata": {},
   "source": [
    "We hope you liked this introduction to GNN's this is a highly compeling subject that is currently on of the most researched on the world. Feel free to contact us to any questions you may have about this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "af92ff48b314d6de65d7d307803e052e27c55ee0db97487118890f19379fef9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
